#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ 

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’è‡ªå‹•æ¤œå‡ºã—ã€
ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚
"""

import json
import logging
import statistics
import time
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import subprocess
import sys

# ãƒ­ã‚°è¨­å®š
logger = logging.getLogger(__name__)


@dataclass
class BenchmarkResult:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    name: str
    timestamp: datetime
    mean_time: float
    std_dev: float
    min_time: float
    max_time: float
    iterations: int
    commit_hash: Optional[str] = None
    branch: Optional[str] = None


@dataclass
class RegressionAlert:
    """å›å¸°ã‚¢ãƒ©ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹"""
    benchmark_name: str
    current_result: BenchmarkResult
    baseline_result: BenchmarkResult
    regression_percentage: float
    severity: str  # 'LOW', 'MEDIUM', 'HIGH', 'CRITICAL'
    timestamp: datetime
    analysis: str


class PerformanceMonitor:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ»å›å¸°æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, data_dir: str = "logs/performance"):
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–

        Args:
            data_dir: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        self.results_file = self.data_dir / "benchmark_results.json"
        self.alerts_file = self.data_dir / "regression_alerts.json"
        
        # å›å¸°æ¤œå‡ºã®é–¾å€¤è¨­å®š
        self.thresholds = {
            'LOW': 5.0,      # 5%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹
            'MEDIUM': 15.0,  # 15%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹
            'HIGH': 30.0,    # 30%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹
            'CRITICAL': 50.0 # 50%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹
        }
        
        # çµ±è¨ˆçš„æœ‰æ„æ€§ã®è¨­å®š
        self.min_samples = 3  # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
        self.confidence_level = 0.95  # ä¿¡é ¼åº¦
        
        logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ")

    def run_benchmarks(self, test_pattern: str = "test_*benchmark*") -> List[BenchmarkResult]:
        """
        ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã€çµæœã‚’åé›†

        Args:
            test_pattern: å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³

        Returns:
            ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­: {test_pattern}")
        
        try:
            # pytest-benchmarkã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œ
            cmd = [
                sys.executable, "-m", "pytest",
                "tests/test_performance_benchmarks.py",
                "-k", test_pattern,
                "--benchmark-json=benchmark_results.json",
                "--benchmark-only",
                "-v"
            ]
            
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=300  # 5åˆ†ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
            )
            
            if result.returncode != 0:
                logger.error(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã«å¤±æ•—: {result.stderr}")
                return []
            
            # çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
            benchmark_file = Path("benchmark_results.json")
            if not benchmark_file.exists():
                logger.warning("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
            
            with open(benchmark_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            # çµæœã‚’è§£æã—ã¦BenchmarkResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            results = []
            commit_hash = self._get_current_commit_hash()
            branch = self._get_current_branch()
            
            for benchmark in data.get('benchmarks', []):
                result = BenchmarkResult(
                    name=benchmark['name'],
                    timestamp=datetime.now(),
                    mean_time=benchmark['stats']['mean'],
                    std_dev=benchmark['stats']['stddev'],
                    min_time=benchmark['stats']['min'],
                    max_time=benchmark['stats']['max'],
                    iterations=benchmark['stats']['rounds'],
                    commit_hash=commit_hash,
                    branch=branch
                )
                results.append(result)
            
            # çµæœã‚’ä¿å­˜
            self._save_results(results)
            
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            benchmark_file.unlink(missing_ok=True)
            
            logger.info(f"{len(results)}å€‹ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’åé›†ã—ã¾ã—ãŸ")
            return results
            
        except subprocess.TimeoutExpired:
            logger.error("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡ŒãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ")
            return []
        except Exception as e:
            logger.error(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return []

    def run_gui_responsiveness_test(self, widget=None, scenarios=None) -> Dict[str, Any]:
        """
        GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ

        Args:
            widget: ãƒ†ã‚¹ãƒˆå¯¾è±¡ã®ã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆ
            scenarios: ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªã®ãƒªã‚¹ãƒˆ

        Returns:
            GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆçµæœ
        """
        logger.info("GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­")
        
        try:
            from scripts.gui_responsiveness_monitor import ResponsivenessMonitor, create_default_test_scenarios
            
            # GUIå¿œç­”æ€§ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–
            gui_monitor = ResponsivenessMonitor(str(self.data_dir / "gui"))
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚·ãƒŠãƒªã‚ªã‚’ä½¿ç”¨ï¼ˆã‚¦ã‚£ã‚¸ã‚§ãƒƒãƒˆãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„å ´åˆï¼‰
            if scenarios is None:
                scenarios = create_default_test_scenarios()
            
            if not scenarios:
                logger.warning("GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
                return {
                    'success': False,
                    'error': 'ãƒ†ã‚¹ãƒˆã‚·ãƒŠãƒªã‚ªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“',
                    'metrics': None
                }
            
            # å¿œç­”æ€§ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
            metrics = gui_monitor.run_responsiveness_test(widget, scenarios)
            
            # çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            logger.info(f"GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆå®Œäº†:")
            logger.info(f"  ç·ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°: {metrics.total_interactions}")
            logger.info(f"  æˆåŠŸã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°: {metrics.successful_interactions}")
            logger.info(f"  å¹³å‡å¿œç­”æ™‚é–“: {metrics.average_response_time:.4f}ç§’")
            logger.info(f"  95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {metrics.p95_response_time:.4f}ç§’")
            
            return {
                'success': True,
                'metrics': metrics,
                'summary': {
                    'total_interactions': metrics.total_interactions,
                    'successful_interactions': metrics.successful_interactions,
                    'failed_interactions': metrics.failed_interactions,
                    'average_response_time': metrics.average_response_time,
                    'p95_response_time': metrics.p95_response_time,
                    'success_rate': (metrics.successful_interactions / metrics.total_interactions * 100) if metrics.total_interactions > 0 else 0
                }
            }
            
        except ImportError as e:
            logger.error(f"GUIå¿œç­”æ€§ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
            return {
                'success': False,
                'error': f'GUIå¿œç­”æ€§ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}',
                'metrics': None
            }
        except Exception as e:
            logger.error(f"GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return {
                'success': False,
                'error': str(e),
                'metrics': None
            }

    def run_memory_profiling_test(self, duration_minutes: int = 5) -> Dict[str, Any]:
        """
        ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ

        Args:
            duration_minutes: ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°æœŸé–“ï¼ˆåˆ†ï¼‰

        Returns:
            ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœ
        """
        logger.info(f"ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œä¸­ï¼ˆæœŸé–“: {duration_minutes}åˆ†ï¼‰")
        
        try:
            import sys
            from pathlib import Path
            sys.path.insert(0, str(Path(__file__).parent.parent))
            from scripts.memory_profiler import MemoryProfiler
            
            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ã‚’åˆæœŸåŒ–
            memory_profiler = MemoryProfiler(str(self.data_dir / "memory"))
            
            # åˆæœŸã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
            initial_snapshot = memory_profiler.take_snapshot("ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆé–‹å§‹")
            
            # çŸ­æ™‚é–“ã®ç›£è¦–ã‚’å®Ÿè¡Œ
            memory_profiler.start_monitoring(interval=1.0)
            
            # æŒ‡å®šæ™‚é–“å¾…æ©Ÿ
            import time
            time.sleep(duration_minutes * 60)
            
            # ç›£è¦–åœæ­¢
            memory_profiler.stop_monitoring()
            
            # æœ€çµ‚ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆ
            final_snapshot = memory_profiler.take_snapshot("ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆçµ‚äº†")
            
            # ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯æ¤œå‡º
            leaks = memory_profiler.detect_memory_leaks(duration_minutes=duration_minutes)
            
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = memory_profiler.generate_memory_report(hours=1)
            
            # çµæœã‚’ãƒ­ã‚°ã«è¨˜éŒ²
            memory_diff = final_snapshot.process_memory_mb - initial_snapshot.process_memory_mb
            logger.info(f"ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Œäº†:")
            logger.info(f"  ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å¤‰åŒ–: {memory_diff:+.1f}MB")
            logger.info(f"  æ¤œå‡ºã•ã‚ŒãŸãƒªãƒ¼ã‚¯æ•°: {len(leaks)}")
            logger.info(f"  ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ•°: {len(memory_profiler.snapshots)}")
            
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            memory_profiler.cleanup()
            
            return {
                'success': True,
                'initial_memory_mb': initial_snapshot.process_memory_mb,
                'final_memory_mb': final_snapshot.process_memory_mb,
                'memory_change_mb': memory_diff,
                'leaks_detected': len(leaks),
                'critical_leaks': len([l for l in leaks if l.severity == 'CRITICAL']),
                'snapshots_count': len(memory_profiler.snapshots),
                'report': report,
                'summary': {
                    'duration_minutes': duration_minutes,
                    'memory_stable': abs(memory_diff) < 10.0,  # 10MBä»¥ä¸‹ã®å¤‰åŒ–ã¯å®‰å®šã¨ã¿ãªã™
                    'no_critical_leaks': len([l for l in leaks if l.severity == 'CRITICAL']) == 0,
                    'monitoring_successful': len(memory_profiler.snapshots) > 0
                }
            }
            
        except ImportError as e:
            logger.error(f"ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
            return {
                'success': False,
                'error': f'ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}',
                'summary': None
            }
        except Exception as e:
            logger.error(f"ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            return {
                'success': False,
                'error': str(e),
                'summary': None
            }

    def detect_regressions(self, current_results: List[BenchmarkResult]) -> List[RegressionAlert]:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’æ¤œå‡º

        Args:
            current_results: ç¾åœ¨ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ

        Returns:
            æ¤œå‡ºã•ã‚ŒãŸå›å¸°ã‚¢ãƒ©ãƒ¼ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã®æ¤œå‡ºã‚’é–‹å§‹")
        
        alerts = []
        historical_results = self._load_historical_results()
        
        for current in current_results:
            baseline = self._get_baseline_result(current.name, historical_results)
            
            if baseline is None:
                logger.info(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {current.name}")
                continue
            
            # å›å¸°ã‚’æ¤œå‡º
            regression_percentage = self._calculate_regression_percentage(
                baseline.mean_time, current.mean_time
            )
            
            if regression_percentage >= self.thresholds['LOW']:
                severity = self._determine_severity(regression_percentage)
                
                # 'NONE'ã®å ´åˆã¯ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ãªã„
                if severity != 'NONE':
                    analysis = self._analyze_regression(current, baseline)
                    
                    alert = RegressionAlert(
                        benchmark_name=current.name,
                        current_result=current,
                        baseline_result=baseline,
                        regression_percentage=regression_percentage,
                        severity=severity,
                        timestamp=datetime.now(),
                        analysis=analysis
                    )
                    
                    alerts.append(alert)
                    logger.warning(
                        f"ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’æ¤œå‡º: {current.name} "
                        f"({regression_percentage:.1f}% ä½ä¸‹, {severity})"
                    )
        
        if alerts:
            self._save_alerts(alerts)
        
        logger.info(f"{len(alerts)}å€‹ã®å›å¸°ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        return alerts

    def generate_performance_report(self, days: int = 30) -> Dict[str, Any]:
        """
        ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            days: ãƒ¬ãƒãƒ¼ãƒˆå¯¾è±¡æœŸé–“ï¼ˆæ—¥æ•°ï¼‰

        Returns:
            ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ
        """
        logger.info(f"éå»{days}æ—¥é–“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­")
        
        cutoff_date = datetime.now() - timedelta(days=days)
        historical_results = self._load_historical_results()
        
        # æœŸé–“å†…ã®çµæœã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        recent_results = [
            r for r in historical_results
            if r.timestamp >= cutoff_date
        ]
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åˆ¥ã®çµ±è¨ˆã‚’è¨ˆç®—
        benchmark_stats = {}
        for result in recent_results:
            if result.name not in benchmark_stats:
                benchmark_stats[result.name] = []
            benchmark_stats[result.name].append(result.mean_time)
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        report = {
            'period': f"{days}æ—¥é–“",
            'total_benchmarks': len(recent_results),
            'unique_benchmarks': len(benchmark_stats),
            'benchmarks': {}
        }
        
        for name, times in benchmark_stats.items():
            if len(times) >= 2:
                trend = self._calculate_trend(times)
                report['benchmarks'][name] = {
                    'samples': len(times),
                    'mean_time': statistics.mean(times),
                    'std_dev': statistics.stdev(times) if len(times) > 1 else 0,
                    'min_time': min(times),
                    'max_time': max(times),
                    'trend': trend
                }
        
        # æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å«ã‚ã‚‹
        recent_alerts = self._load_recent_alerts(days)
        report['alerts'] = len(recent_alerts)
        report['critical_alerts'] = len([a for a in recent_alerts if a.severity == 'CRITICAL'])
        
        logger.info("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã—ãŸ")
        return report

    def _save_results(self, results: List[BenchmarkResult]) -> None:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ä¿å­˜"""
        historical_results = self._load_historical_results()
        historical_results.extend(results)
        
        # å¤ã„çµæœã‚’å‰Šé™¤ï¼ˆ90æ—¥ä»¥ä¸Šå‰ï¼‰
        cutoff_date = datetime.now() - timedelta(days=90)
        historical_results = [
            r for r in historical_results
            if r.timestamp >= cutoff_date
        ]
        
        # JSONå½¢å¼ã§ä¿å­˜
        data = [asdict(r) for r in historical_results]
        for item in data:
            item['timestamp'] = item['timestamp'].isoformat()
        
        with open(self.results_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _load_historical_results(self) -> List[BenchmarkResult]:
        """éå»ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’èª­ã¿è¾¼ã¿"""
        if not self.results_file.exists():
            return []
        
        try:
            with open(self.results_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            results = []
            for item in data:
                item['timestamp'] = datetime.fromisoformat(item['timestamp'])
                results.append(BenchmarkResult(**item))
            
            return results
        except Exception as e:
            logger.error(f"éå»ã®çµæœèª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return []

    def _get_baseline_result(self, benchmark_name: str, 
                           historical_results: List[BenchmarkResult]) -> Optional[BenchmarkResult]:
        """ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã‚’å–å¾—"""
        # åŒã˜ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®éå»ã®çµæœã‚’å–å¾—
        same_benchmark = [
            r for r in historical_results
            if r.name == benchmark_name
        ]
        
        if len(same_benchmark) < self.min_samples:
            return None
        
        # æœ€æ–°ã®å®‰å®šã—ãŸçµæœã‚’ä½¿ç”¨ï¼ˆéå»7æ—¥é–“ã®å¹³å‡ï¼‰
        cutoff_date = datetime.now() - timedelta(days=7)
        recent_results = [
            r for r in same_benchmark
            if r.timestamp >= cutoff_date
        ]
        
        if len(recent_results) < self.min_samples:
            # éå»7æ—¥é–“ã«ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã‚‹å ´åˆã¯ã€æœ€æ–°ã®çµæœã‚’ä½¿ç”¨
            sorted_results = sorted(same_benchmark, key=lambda x: x.timestamp, reverse=True)
            recent_results = sorted_results[:self.min_samples]
        
        if len(recent_results) < self.min_samples:
            return None
        
        # å¹³å‡å€¤ã‚’è¨ˆç®—ã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã™ã‚‹
        mean_time = statistics.mean([r.mean_time for r in recent_results])
        std_dev = statistics.stdev([r.mean_time for r in recent_results]) if len(recent_results) > 1 else 0
        
        # ä»£è¡¨çš„ãªçµæœã‚’ä½œæˆ
        latest = recent_results[0]  # æœ€æ–°ã®çµæœ
        return BenchmarkResult(
            name=benchmark_name,
            timestamp=latest.timestamp,
            mean_time=mean_time,
            std_dev=std_dev,
            min_time=min([r.min_time for r in recent_results]),
            max_time=max([r.max_time for r in recent_results]),
            iterations=latest.iterations,
            commit_hash=latest.commit_hash,
            branch=latest.branch
        )

    def _calculate_regression_percentage(self, baseline: float, current: float) -> float:
        """å›å¸°ç‡ã‚’è¨ˆç®—"""
        if baseline == 0:
            return 0.0
        return ((current - baseline) / baseline) * 100

    def _determine_severity(self, regression_percentage: float) -> str:
        """å›å¸°ã®é‡è¦åº¦ã‚’åˆ¤å®š"""
        if regression_percentage >= self.thresholds['CRITICAL']:
            return 'CRITICAL'
        elif regression_percentage >= self.thresholds['HIGH']:
            return 'HIGH'
        elif regression_percentage >= self.thresholds['MEDIUM']:
            return 'MEDIUM'
        elif regression_percentage >= self.thresholds['LOW']:
            return 'LOW'
        else:
            # é–¾å€¤æœªæº€ã®å ´åˆã¯å›å¸°ã¨ã—ã¦æ‰±ã‚ãªã„
            return 'NONE'

    def _analyze_regression(self, current: BenchmarkResult, baseline: BenchmarkResult) -> str:
        """å›å¸°ã®åŸå› åˆ†æ"""
        analysis_parts = []
        
        # å®Ÿè¡Œæ™‚é–“ã®å¤‰åŒ–
        time_change = current.mean_time - baseline.mean_time
        analysis_parts.append(f"å®Ÿè¡Œæ™‚é–“ãŒ{time_change:.3f}ç§’å¢—åŠ ")
        
        # æ¨™æº–åå·®ã®å¤‰åŒ–
        if current.std_dev > baseline.std_dev * 1.5:
            analysis_parts.append("å®Ÿè¡Œæ™‚é–“ã®ã°ã‚‰ã¤ããŒå¤§å¹…ã«å¢—åŠ ")
        
        # æœ€å¤§å®Ÿè¡Œæ™‚é–“ã®å¤‰åŒ–
        if current.max_time > baseline.max_time * 1.3:
            analysis_parts.append("æœ€å¤§å®Ÿè¡Œæ™‚é–“ãŒå¤§å¹…ã«å¢—åŠ ")
        
        # ã‚³ãƒŸãƒƒãƒˆæƒ…å ±
        if current.commit_hash and baseline.commit_hash:
            if current.commit_hash != baseline.commit_hash:
                analysis_parts.append(f"ã‚³ãƒŸãƒƒãƒˆå¤‰æ›´: {baseline.commit_hash[:8]} â†’ {current.commit_hash[:8]}")
        
        return "ã€".join(analysis_parts)

    def _calculate_trend(self, times: List[float]) -> str:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’è¨ˆç®—"""
        if len(times) < 2:
            return "ä¸æ˜"
        
        # ç·šå½¢å›å¸°ã®å‚¾ãã‚’è¨ˆç®—
        n = len(times)
        x = list(range(n))
        
        sum_x = sum(x)
        sum_y = sum(times)
        sum_xy = sum(x[i] * times[i] for i in range(n))
        sum_x2 = sum(x[i] ** 2 for i in range(n))
        
        slope = (n * sum_xy - sum_x * sum_y) / (n * sum_x2 - sum_x ** 2)
        
        if slope > 0.001:
            return "æ‚ªåŒ–"
        elif slope < -0.001:
            return "æ”¹å–„"
        else:
            return "å®‰å®š"

    def _save_alerts(self, alerts: List[RegressionAlert]) -> None:
        """ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ä¿å­˜"""
        historical_alerts = self._load_all_alerts()
        historical_alerts.extend(alerts)
        
        # å¤ã„ã‚¢ãƒ©ãƒ¼ãƒˆã‚’å‰Šé™¤ï¼ˆ30æ—¥ä»¥ä¸Šå‰ï¼‰
        cutoff_date = datetime.now() - timedelta(days=30)
        historical_alerts = [
            a for a in historical_alerts
            if a.timestamp >= cutoff_date
        ]
        
        # JSONå½¢å¼ã§ä¿å­˜
        data = []
        for alert in historical_alerts:
            alert_dict = asdict(alert)
            alert_dict['timestamp'] = alert_dict['timestamp'].isoformat()
            alert_dict['current_result']['timestamp'] = alert_dict['current_result']['timestamp'].isoformat()
            alert_dict['baseline_result']['timestamp'] = alert_dict['baseline_result']['timestamp'].isoformat()
            data.append(alert_dict)
        
        with open(self.alerts_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)

    def _load_all_alerts(self) -> List[RegressionAlert]:
        """ã™ã¹ã¦ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        if not self.alerts_file.exists():
            return []
        
        try:
            with open(self.alerts_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            alerts = []
            for item in data:
                item['timestamp'] = datetime.fromisoformat(item['timestamp'])
                item['current_result']['timestamp'] = datetime.fromisoformat(item['current_result']['timestamp'])
                item['baseline_result']['timestamp'] = datetime.fromisoformat(item['baseline_result']['timestamp'])
                
                current_result = BenchmarkResult(**item['current_result'])
                baseline_result = BenchmarkResult(**item['baseline_result'])
                
                alert = RegressionAlert(
                    benchmark_name=item['benchmark_name'],
                    current_result=current_result,
                    baseline_result=baseline_result,
                    regression_percentage=item['regression_percentage'],
                    severity=item['severity'],
                    timestamp=item['timestamp'],
                    analysis=item['analysis']
                )
                alerts.append(alert)
            
            return alerts
        except Exception as e:
            logger.error(f"ã‚¢ãƒ©ãƒ¼ãƒˆèª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return []

    def _load_recent_alerts(self, days: int) -> List[RegressionAlert]:
        """æœ€è¿‘ã®ã‚¢ãƒ©ãƒ¼ãƒˆã‚’èª­ã¿è¾¼ã¿"""
        all_alerts = self._load_all_alerts()
        cutoff_date = datetime.now() - timedelta(days=days)
        
        return [
            alert for alert in all_alerts
            if alert.timestamp >= cutoff_date
        ]

    def _get_current_commit_hash(self) -> Optional[str]:
        """ç¾åœ¨ã®ã‚³ãƒŸãƒƒãƒˆãƒãƒƒã‚·ãƒ¥ã‚’å–å¾—"""
        try:
            result = subprocess.run(
                ['git', 'rev-parse', 'HEAD'],
                capture_output=True,
                text=True,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception:
            pass
        return None

    def _get_current_branch(self) -> Optional[str]:
        """ç¾åœ¨ã®ãƒ–ãƒ©ãƒ³ãƒåã‚’å–å¾—"""
        try:
            result = subprocess.run(
                ['git', 'branch', '--show-current'],
                capture_output=True,
                text=True,
                timeout=10
            )
            if result.returncode == 0:
                return result.stdout.strip()
        except Exception:
            pass
        return None


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ")
    parser.add_argument(
        '--run-benchmarks',
        action='store_true',
        help='ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ'
    )
    parser.add_argument(
        '--detect-regressions',
        action='store_true',
        help='å›å¸°æ¤œå‡ºã®ã¿å®Ÿè¡Œ'
    )
    parser.add_argument(
        '--generate-report',
        type=int,
        metavar='DAYS',
        help='æŒ‡å®šæ—¥æ•°ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ'
    )
    parser.add_argument(
        '--test-pattern',
        default='test_*benchmark*',
        help='å®Ÿè¡Œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³'
    )
    parser.add_argument(
        '--data-dir',
        default='logs/performance',
        help='ãƒ‡ãƒ¼ã‚¿ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª'
    )
    parser.add_argument(
        '--memory-profile',
        type=int,
        metavar='MINUTES',
        help='æŒ‡å®šåˆ†æ•°é–“ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’å®Ÿè¡Œ'
    )
    parser.add_argument(
        '--verbose',
        action='store_true',
        help='è©³ç´°ãƒ­ã‚°ã‚’å‡ºåŠ›'
    )
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°è¨­å®š
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('logs/performance_monitor.log', encoding='utf-8')
        ]
    )
    
    monitor = PerformanceMonitor(args.data_dir)
    
    try:
        if args.run_benchmarks:
            # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã¨å›å¸°æ¤œå‡º
            results = monitor.run_benchmarks(args.test_pattern)
            if results:
                alerts = monitor.detect_regressions(results)
                
                if alerts:
                    print(f"\nâš ï¸  {len(alerts)}å€‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’æ¤œå‡ºã—ã¾ã—ãŸ:")
                    for alert in alerts:
                        print(f"  - {alert.benchmark_name}: {alert.regression_percentage:.1f}% ä½ä¸‹ ({alert.severity})")
                        print(f"    åˆ†æ: {alert.analysis}")
                else:
                    print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            else:
                print("âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        
        elif args.detect_regressions:
            # æ—¢å­˜çµæœã‹ã‚‰å›å¸°æ¤œå‡ºã®ã¿
            historical_results = monitor._load_historical_results()
            if historical_results:
                # æœ€æ–°ã®çµæœã‚’ä½¿ç”¨
                latest_results = {}
                for result in historical_results:
                    if result.name not in latest_results or result.timestamp > latest_results[result.name].timestamp:
                        latest_results[result.name] = result
                
                alerts = monitor.detect_regressions(list(latest_results.values()))
                
                if alerts:
                    print(f"\nâš ï¸  {len(alerts)}å€‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’æ¤œå‡ºã—ã¾ã—ãŸ:")
                    for alert in alerts:
                        print(f"  - {alert.benchmark_name}: {alert.regression_percentage:.1f}% ä½ä¸‹ ({alert.severity})")
                else:
                    print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            else:
                print("âŒ éå»ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        
        elif args.memory_profile:
            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ
            memory_result = monitor.run_memory_profiling_test(duration_minutes=args.memory_profile)
            
            if memory_result['success']:
                print(f"\nğŸ§  ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœ ({args.memory_profile}åˆ†é–“)")
                print(f"åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_result['initial_memory_mb']:.1f}MB")
                print(f"æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_result['final_memory_mb']:.1f}MB")
                print(f"ãƒ¡ãƒ¢ãƒªå¤‰åŒ–: {memory_result['memory_change_mb']:+.1f}MB")
                print(f"æ¤œå‡ºã•ã‚ŒãŸãƒªãƒ¼ã‚¯æ•°: {memory_result['leaks_detected']}")
                print(f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆæ•°: {memory_result['snapshots_count']}")
                
                if memory_result['critical_leaks'] > 0:
                    print(f"\nâš ï¸ é‡è¦ãªãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯: {memory_result['critical_leaks']}å€‹")
                    sys.exit(1)
                
                summary = memory_result['summary']
                if summary['memory_stable'] and summary['no_critical_leaks']:
                    print("\nâœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯å®‰å®šã—ã¦ã„ã¾ã™")
                else:
                    print("\nâš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                    if not summary['memory_stable']:
                        print(f"  - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒä¸å®‰å®š: {memory_result['memory_change_mb']:+.1f}MB")
                    if not summary['no_critical_leaks']:
                        print(f"  - é‡è¦ãªãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ãŒæ¤œå‡º: {memory_result['critical_leaks']}å€‹")
            else:
                print(f"âŒ ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã«å¤±æ•—: {memory_result['error']}")
                sys.exit(1)
        
        elif args.generate_report is not None:
            # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
            report = monitor.generate_performance_report(args.generate_report)
            
            print(f"\nğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆ ({report['period']})")
            print(f"ç·ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ•°: {report['total_benchmarks']}")
            print(f"ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ•°: {report['unique_benchmarks']}")
            print(f"ã‚¢ãƒ©ãƒ¼ãƒˆæ•°: {report['alerts']} (é‡è¦: {report['critical_alerts']})")
            
            if report['benchmarks']:
                print("\nãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è©³ç´°:")
                for name, stats in report['benchmarks'].items():
                    trend_emoji = {"æ”¹å–„": "ğŸ“ˆ", "æ‚ªåŒ–": "ğŸ“‰", "å®‰å®š": "â¡ï¸", "ä¸æ˜": "â“"}
                    print(f"  {name}:")
                    print(f"    å¹³å‡å®Ÿè¡Œæ™‚é–“: {stats['mean_time']:.3f}ç§’")
                    print(f"    ãƒˆãƒ¬ãƒ³ãƒ‰: {trend_emoji.get(stats['trend'], 'â“')} {stats['trend']}")
                    print(f"    ã‚µãƒ³ãƒ—ãƒ«æ•°: {stats['samples']}")
            
            # GUIå¿œç­”æ€§ãƒ¬ãƒãƒ¼ãƒˆã‚‚ç”Ÿæˆ
            try:
                gui_result = monitor.run_gui_responsiveness_test()
                if gui_result['success']:
                    print(f"\nğŸ–±ï¸ GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆçµæœ:")
                    summary = gui_result['summary']
                    print(f"ç·ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³æ•°: {summary['total_interactions']}")
                    print(f"æˆåŠŸç‡: {summary['success_rate']:.1f}%")
                    print(f"å¹³å‡å¿œç­”æ™‚é–“: {summary['average_response_time']:.4f}ç§’")
                    print(f"95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«: {summary['p95_response_time']:.4f}ç§’")
                else:
                    print(f"\nâš ï¸ GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—: {gui_result['error']}")
            except Exception as e:
                print(f"\nâš ï¸ GUIå¿œç­”æ€§ãƒ†ã‚¹ãƒˆã§ã‚¨ãƒ©ãƒ¼: {e}")
            
            # ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆã‚‚ç”Ÿæˆ
            try:
                memory_result = monitor.run_memory_profiling_test(duration_minutes=1)
                if memory_result['success']:
                    print(f"\nğŸ§  ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœ:")
                    print(f"åˆæœŸãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_result['initial_memory_mb']:.1f}MB")
                    print(f"æœ€çµ‚ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_result['final_memory_mb']:.1f}MB")
                    print(f"ãƒ¡ãƒ¢ãƒªå¤‰åŒ–: {memory_result['memory_change_mb']:+.1f}MB")
                    print(f"æ¤œå‡ºã•ã‚ŒãŸãƒªãƒ¼ã‚¯æ•°: {memory_result['leaks_detected']}")
                    if memory_result['critical_leaks'] > 0:
                        print(f"âš ï¸ é‡è¦ãªãƒªãƒ¼ã‚¯: {memory_result['critical_leaks']}å€‹")
                    
                    summary = memory_result['summary']
                    if summary['memory_stable'] and summary['no_critical_leaks']:
                        print("âœ… ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯å®‰å®šã—ã¦ã„ã¾ã™")
                    else:
                        print("âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«å•é¡ŒãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ")
                else:
                    print(f"\nâš ï¸ ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’ã‚¹ã‚­ãƒƒãƒ—: {memory_result['error']}")
            except Exception as e:
                print(f"\nâš ï¸ ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã§ã‚¨ãƒ©ãƒ¼: {e}")
        
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
            results = monitor.run_benchmarks(args.test_pattern)
            if results:
                alerts = monitor.detect_regressions(results)
                
                if alerts:
                    print(f"\nâš ï¸  {len(alerts)}å€‹ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã‚’æ¤œå‡ºã—ã¾ã—ãŸ:")
                    for alert in alerts:
                        print(f"  - {alert.benchmark_name}: {alert.regression_percentage:.1f}% ä½ä¸‹ ({alert.severity})")
                        print(f"    åˆ†æ: {alert.analysis}")
                    
                    # é‡è¦ãªã‚¢ãƒ©ãƒ¼ãƒˆãŒã‚ã‚‹å ´åˆã¯çµ‚äº†ã‚³ãƒ¼ãƒ‰1ã§çµ‚äº†
                    critical_alerts = [a for a in alerts if a.severity in ['HIGH', 'CRITICAL']]
                    if critical_alerts:
                        sys.exit(1)
                else:
                    print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å›å¸°ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            else:
                print("âŒ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                sys.exit(1)
    
    except KeyboardInterrupt:
        logger.info("ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        sys.exit(1)
    except Exception as e:
        logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()