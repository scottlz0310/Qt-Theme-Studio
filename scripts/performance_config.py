#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ è¨­å®š

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã®è¨­å®šã‚’ç®¡ç†ã—ã¾ã™ã€‚
"""

import json
import logging
from dataclasses import dataclass, asdict
from pathlib import Path
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)


@dataclass
class ThresholdConfig:
    """å›å¸°æ¤œå‡ºé–¾å€¤è¨­å®š"""
    low: float = 5.0      # 5%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹
    medium: float = 15.0  # 15%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹
    high: float = 30.0    # 30%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹
    critical: float = 50.0 # 50%ä»¥ä¸Šã®æ€§èƒ½ä½ä¸‹


@dataclass
class BenchmarkConfig:
    """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š"""
    name: str
    test_pattern: str
    timeout: int = 300  # 5åˆ†
    min_iterations: int = 3
    max_iterations: int = 10
    warmup_iterations: int = 1
    enabled: bool = True
    custom_thresholds: Optional[ThresholdConfig] = None


@dataclass
class AlertConfig:
    """ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š"""
    enabled: bool = True
    email_notifications: bool = False
    slack_webhook: Optional[str] = None
    github_issues: bool = False
    severity_levels: List[str] = None
    
    def __post_init__(self):
        if self.severity_levels is None:
            self.severity_levels = ['MEDIUM', 'HIGH', 'CRITICAL']


@dataclass
class PerformanceConfig:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“è¨­å®š"""
    data_dir: str = "logs/performance"
    retention_days: int = 90
    min_samples: int = 3
    confidence_level: float = 0.95
    
    default_thresholds: ThresholdConfig = None
    benchmarks: List[BenchmarkConfig] = None
    alerts: AlertConfig = None
    
    def __post_init__(self):
        if self.default_thresholds is None:
            self.default_thresholds = ThresholdConfig()
        
        if self.benchmarks is None:
            self.benchmarks = self._get_default_benchmarks()
        
        if self.alerts is None:
            self.alerts = AlertConfig()
    
    def _get_default_benchmarks(self) -> List[BenchmarkConfig]:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã‚’å–å¾—"""
        return [
            BenchmarkConfig(
                name="theme_loading",
                test_pattern="test_theme_loading_benchmark",
                timeout=60,
                min_iterations=5
            ),
            BenchmarkConfig(
                name="large_theme_loading", 
                test_pattern="test_large_theme_loading_benchmark",
                timeout=120,
                min_iterations=3
            ),
            BenchmarkConfig(
                name="theme_validation",
                test_pattern="test_theme_validation_benchmark",
                timeout=30,
                min_iterations=5
            ),
            BenchmarkConfig(
                name="css_generation",
                test_pattern="test_css_generation_benchmark",
                timeout=60,
                min_iterations=3
            ),
            BenchmarkConfig(
                name="theme_export",
                test_pattern="test_theme_export_benchmark",
                timeout=60,
                min_iterations=3
            ),
            BenchmarkConfig(
                name="file_operations",
                test_pattern="test_file_*_benchmark",
                timeout=30,
                min_iterations=5
            ),
            BenchmarkConfig(
                name="memory_usage",
                test_pattern="test_*_memory_*_benchmark",
                timeout=120,
                min_iterations=3,
                custom_thresholds=ThresholdConfig(
                    low=10.0,    # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¯10%ã‹ã‚‰è­¦å‘Š
                    medium=25.0,
                    high=50.0,
                    critical=75.0
                )
            )
        ]


class PerformanceConfigManager:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è¨­å®šç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config_path: str = ".kiro/performance/config.json"):
        """
        è¨­å®šç®¡ç†ã‚’åˆæœŸåŒ–
        
        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        self.config_path = Path(config_path)
        self.config_path.parent.mkdir(parents=True, exist_ok=True)
        self._config: Optional[PerformanceConfig] = None
    
    def load_config(self) -> PerformanceConfig:
        """è¨­å®šã‚’èª­ã¿è¾¼ã¿"""
        if self._config is not None:
            return self._config
        
        if self.config_path.exists():
            try:
                with open(self.config_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # JSONã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã«å¤‰æ›
                config_dict = self._convert_from_json(data)
                self._config = PerformanceConfig(**config_dict)
                
                logger.info(f"è¨­å®šã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {self.config_path}")
                return self._config
                
            except Exception as e:
                logger.error(f"è¨­å®šèª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
                logger.info("ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½œæˆ
        self._config = PerformanceConfig()
        self.save_config()
        return self._config
    
    def save_config(self) -> None:
        """è¨­å®šã‚’ä¿å­˜"""
        if self._config is None:
            return
        
        try:
            # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã‚’JSONã«å¤‰æ›
            data = self._convert_to_json(asdict(self._config))
            
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"è¨­å®šã‚’ä¿å­˜ã—ã¾ã—ãŸ: {self.config_path}")
            
        except Exception as e:
            logger.error(f"è¨­å®šä¿å­˜ã«å¤±æ•—: {e}")
    
    def get_benchmark_config(self, name: str) -> Optional[BenchmarkConfig]:
        """æŒ‡å®šã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã‚’å–å¾—"""
        config = self.load_config()
        
        for benchmark in config.benchmarks:
            if benchmark.name == name:
                return benchmark
        
        return None
    
    def update_benchmark_config(self, name: str, updates: Dict[str, Any]) -> bool:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã‚’æ›´æ–°"""
        config = self.load_config()
        
        for i, benchmark in enumerate(config.benchmarks):
            if benchmark.name == name:
                # æ›´æ–°ã‚’é©ç”¨
                for key, value in updates.items():
                    if hasattr(benchmark, key):
                        setattr(benchmark, key, value)
                
                self.save_config()
                logger.info(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã‚’æ›´æ–°: {name}")
                return True
        
        logger.warning(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {name}")
        return False
    
    def add_benchmark_config(self, benchmark: BenchmarkConfig) -> None:
        """æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã‚’è¿½åŠ """
        config = self.load_config()
        
        # æ—¢å­˜ã®è¨­å®šã‚’ãƒã‚§ãƒƒã‚¯
        existing_names = [b.name for b in config.benchmarks]
        if benchmark.name in existing_names:
            logger.warning(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šãŒæ—¢ã«å­˜åœ¨ã—ã¾ã™: {benchmark.name}")
            return
        
        config.benchmarks.append(benchmark)
        self.save_config()
        logger.info(f"æ–°ã—ã„ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã‚’è¿½åŠ : {benchmark.name}")
    
    def remove_benchmark_config(self, name: str) -> bool:
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã‚’å‰Šé™¤"""
        config = self.load_config()
        
        original_count = len(config.benchmarks)
        config.benchmarks = [b for b in config.benchmarks if b.name != name]
        
        if len(config.benchmarks) < original_count:
            self.save_config()
            logger.info(f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã‚’å‰Šé™¤: {name}")
            return True
        
        logger.warning(f"å‰Šé™¤å¯¾è±¡ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {name}")
        return False
    
    def get_thresholds_for_benchmark(self, benchmark_name: str) -> ThresholdConfig:
        """æŒ‡å®šã•ã‚ŒãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®é–¾å€¤è¨­å®šã‚’å–å¾—"""
        benchmark = self.get_benchmark_config(benchmark_name)
        
        if benchmark and benchmark.custom_thresholds:
            return benchmark.custom_thresholds
        
        config = self.load_config()
        return config.default_thresholds
    
    def validate_config(self) -> List[str]:
        """è¨­å®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        config = self.load_config()
        errors = []
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¤œè¨¼
        if not config.data_dir:
            errors.append("data_dirãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # ä¿æŒæœŸé–“ã®æ¤œè¨¼
        if config.retention_days <= 0:
            errors.append("retention_daysã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°ã®æ¤œè¨¼
        if config.min_samples <= 0:
            errors.append("min_samplesã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # ä¿¡é ¼åº¦ã®æ¤œè¨¼
        if not (0 < config.confidence_level < 1):
            errors.append("confidence_levelã¯0ã¨1ã®é–“ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®šã®æ¤œè¨¼
        benchmark_names = set()
        for benchmark in config.benchmarks:
            if not benchmark.name:
                errors.append("ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯åãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                continue
            
            if benchmark.name in benchmark_names:
                errors.append(f"é‡è¤‡ã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å: {benchmark.name}")
            benchmark_names.add(benchmark.name)
            
            if not benchmark.test_pattern:
                errors.append(f"ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“: {benchmark.name}")
            
            if benchmark.timeout <= 0:
                errors.append(f"ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {benchmark.name}")
            
            if benchmark.min_iterations <= 0:
                errors.append(f"æœ€å°åå¾©å›æ•°ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™: {benchmark.name}")
        
        # é–¾å€¤è¨­å®šã®æ¤œè¨¼
        def validate_thresholds(thresholds: ThresholdConfig, context: str):
            if thresholds.low < 0:
                errors.append(f"{context}: lowé–¾å€¤ã¯éè² ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
            if thresholds.medium <= thresholds.low:
                errors.append(f"{context}: mediumé–¾å€¤ã¯lowé–¾å€¤ã‚ˆã‚Šå¤§ãã„å¿…è¦ãŒã‚ã‚Šã¾ã™")
            if thresholds.high <= thresholds.medium:
                errors.append(f"{context}: highé–¾å€¤ã¯mediumé–¾å€¤ã‚ˆã‚Šå¤§ãã„å¿…è¦ãŒã‚ã‚Šã¾ã™")
            if thresholds.critical <= thresholds.high:
                errors.append(f"{context}: criticalé–¾å€¤ã¯highé–¾å€¤ã‚ˆã‚Šå¤§ãã„å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        validate_thresholds(config.default_thresholds, "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤")
        
        for benchmark in config.benchmarks:
            if benchmark.custom_thresholds:
                validate_thresholds(
                    benchmark.custom_thresholds, 
                    f"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ '{benchmark.name}' ã®ã‚«ã‚¹ã‚¿ãƒ é–¾å€¤"
                )
        
        return errors
    
    def _convert_to_json(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ã‚’JSONå½¢å¼ã«å¤‰æ›"""
        if isinstance(data, dict):
            result = {}
            for key, value in data.items():
                if isinstance(value, dict):
                    result[key] = self._convert_to_json(value)
                elif isinstance(value, list):
                    result[key] = [self._convert_to_json(item) if isinstance(item, dict) else item for item in value]
                else:
                    result[key] = value
            return result
        return data
    
    def _convert_from_json(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """JSONå½¢å¼ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ©ã‚¹ç”¨ã«å¤‰æ›"""
        result = {}
        
        for key, value in data.items():
            if key == 'default_thresholds' and isinstance(value, dict):
                result[key] = ThresholdConfig(**value)
            elif key == 'benchmarks' and isinstance(value, list):
                benchmarks = []
                for item in value:
                    if isinstance(item, dict):
                        # custom_thresholdsã®å‡¦ç†
                        if 'custom_thresholds' in item and item['custom_thresholds']:
                            item['custom_thresholds'] = ThresholdConfig(**item['custom_thresholds'])
                        benchmarks.append(BenchmarkConfig(**item))
                result[key] = benchmarks
            elif key == 'alerts' and isinstance(value, dict):
                result[key] = AlertConfig(**value)
            else:
                result[key] = value
        
        return result


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–è¨­å®šç®¡ç†")
    parser.add_argument(
        '--config-path',
        default='.kiro/performance/config.json',
        help='è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹'
    )
    parser.add_argument(
        '--validate',
        action='store_true',
        help='è¨­å®šã®å¦¥å½“æ€§ã‚’æ¤œè¨¼'
    )
    parser.add_argument(
        '--show-config',
        action='store_true',
        help='ç¾åœ¨ã®è¨­å®šã‚’è¡¨ç¤º'
    )
    parser.add_argument(
        '--create-default',
        action='store_true',
        help='ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ'
    )
    
    args = parser.parse_args()
    
    # ãƒ­ã‚°è¨­å®š
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    manager = PerformanceConfigManager(args.config_path)
    
    try:
        if args.create_default:
            config = PerformanceConfig()
            manager._config = config
            manager.save_config()
            print(f"âœ… ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã—ãŸ: {args.config_path}")
        
        elif args.validate:
            errors = manager.validate_config()
            if errors:
                print("âŒ è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™:")
                for error in errors:
                    print(f"  - {error}")
            else:
                print("âœ… è¨­å®šã¯æ­£å¸¸ã§ã™")
        
        elif args.show_config:
            config = manager.load_config()
            print("ğŸ“‹ ç¾åœ¨ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–è¨­å®š:")
            print(f"  ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {config.data_dir}")
            print(f"  ä¿æŒæœŸé–“: {config.retention_days}æ—¥")
            print(f"  æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°: {config.min_samples}")
            print(f"  ä¿¡é ¼åº¦: {config.confidence_level}")
            
            print("\nğŸ¯ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé–¾å€¤:")
            print(f"  LOW: {config.default_thresholds.low}%")
            print(f"  MEDIUM: {config.default_thresholds.medium}%")
            print(f"  HIGH: {config.default_thresholds.high}%")
            print(f"  CRITICAL: {config.default_thresholds.critical}%")
            
            print(f"\nğŸ§ª ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯è¨­å®š ({len(config.benchmarks)}ä»¶):")
            for benchmark in config.benchmarks:
                status = "æœ‰åŠ¹" if benchmark.enabled else "ç„¡åŠ¹"
                print(f"  - {benchmark.name} ({status})")
                print(f"    ãƒ‘ã‚¿ãƒ¼ãƒ³: {benchmark.test_pattern}")
                print(f"    ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {benchmark.timeout}ç§’")
                if benchmark.custom_thresholds:
                    print(f"    ã‚«ã‚¹ã‚¿ãƒ é–¾å€¤: ã‚ã‚Š")
        
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: è¨­å®šã‚’èª­ã¿è¾¼ã‚“ã§æ¤œè¨¼
            config = manager.load_config()
            errors = manager.validate_config()
            
            if errors:
                print("âš ï¸ è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™:")
                for error in errors:
                    print(f"  - {error}")
            else:
                print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–è¨­å®šãŒæ­£å¸¸ã«èª­ã¿è¾¼ã¾ã‚Œã¾ã—ãŸ")
                print(f"  ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ•°: {len(config.benchmarks)}")
                enabled_count = sum(1 for b in config.benchmarks if b.enabled)
                print(f"  æœ‰åŠ¹ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯: {enabled_count}")
    
    except Exception as e:
        logger.error(f"è¨­å®šç®¡ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main())