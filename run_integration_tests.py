#!/usr/bin/env python3
"""
çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Qt-Theme-Studioã®çµ±åˆãƒ†ã‚¹ãƒˆã¨ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
"""

import sys
import os
import subprocess
import argparse
from pathlib import Path
import json
import time
from datetime import datetime


def setup_test_environment():
    """ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    print("ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ä¸­...")
    
    # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    test_dirs = [
        'tests/fixtures',
        'tests/temp',
        'tests/reports'
    ]
    
    for test_dir in test_dirs:
        Path(test_dir).mkdir(parents=True, exist_ok=True)
    
    # ãƒ†ã‚¹ãƒˆç”¨ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
    test_config = {
        'test_mode': True,
        'log_level': 'DEBUG',
        'temp_dir': 'tests/temp',
        'fixtures_dir': 'tests/fixtures'
    }
    
    config_file = Path('tests/test_config.json')
    with open(config_file, 'w', encoding='utf-8') as f:
        json.dump(test_config, f, indent=2, ensure_ascii=False)
    
    print("âœ“ ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")


def run_pytest_command(test_files, options=None):
    """pytestã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œ"""
    cmd = ['python', '-m', 'pytest']
    
    # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ 
    if isinstance(test_files, str):
        cmd.append(test_files)
    else:
        cmd.extend(test_files)
    
    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ 
    if options:
        cmd.extend(options)
    
    print(f"å®Ÿè¡Œä¸­: {' '.join(cmd)}")
    
    try:
        result = subprocess.run(cmd, capture_output=True, text=True, encoding='utf-8')
        return result
    except Exception as e:
        print(f"ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        return None


def run_complete_workflow_tests():
    """å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("\n=== å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ===")
    
    test_file = 'tests/test_complete_workflow.py'
    options = [
        '-v',
        '--tb=short',
        '--durations=10',
        f'--junitxml=tests/reports/workflow_tests.xml'
    ]
    
    result = run_pytest_command(test_file, options)
    
    if result and result.returncode == 0:
        print("âœ“ å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ")
        return True
    else:
        print("âœ— å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        if result:
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
        return False


def run_accessibility_tests():
    """ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("\n=== ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ===")
    
    test_file = 'tests/test_accessibility_features.py'
    options = [
        '-v',
        '--tb=short',
        '--durations=10',
        f'--junitxml=tests/reports/accessibility_tests.xml'
    ]
    
    result = run_pytest_command(test_file, options)
    
    if result and result.returncode == 0:
        print("âœ“ ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ")
        return True
    else:
        print("âœ— ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        if result:
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
        return False


def run_integration_suite():
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®å®Ÿè¡Œ"""
    print("\n=== çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆå®Ÿè¡Œ ===")
    
    test_file = 'tests/test_integration_suite.py'
    options = [
        '-v',
        '--tb=short',
        '--durations=10',
        f'--junitxml=tests/reports/integration_suite.xml'
    ]
    
    result = run_pytest_command(test_file, options)
    
    if result and result.returncode == 0:
        print("âœ“ çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ")
        return True
    else:
        print("âœ— çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        if result:
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
        return False


def run_all_integration_tests():
    """ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("\n=== ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ===")
    
    test_files = [
        'tests/test_complete_workflow.py',
        'tests/test_accessibility_features.py',
        'tests/test_config_integration.py',
        'tests/test_preview_integration.py',
        'tests/test_integration_suite.py'
    ]
    
    options = [
        '-v',
        '--tb=short',
        '--durations=20',
        '--cov=qt_theme_studio',
        '--cov-report=html:tests/reports/coverage',
        '--cov-report=xml:tests/reports/coverage.xml',
        f'--junitxml=tests/reports/all_integration_tests.xml'
    ]
    
    result = run_pytest_command(test_files, options)
    
    if result and result.returncode == 0:
        print("âœ“ ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ")
        return True
    else:
        print("âœ— ä¸€éƒ¨ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        if result:
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
        return False


def run_performance_tests():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("\n=== ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ ===")
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆç”¨ã®ãƒãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨
    options = [
        '-v',
        '-m', 'performance',
        '--tb=short',
        '--durations=10',
        f'--junitxml=tests/reports/performance_tests.xml'
    ]
    
    result = run_pytest_command('tests/', options)
    
    if result and result.returncode == 0:
        print("âœ“ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸ")
        return True
    else:
        print("âœ— ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
        if result:
            print("STDOUT:", result.stdout)
            print("STDERR:", result.stderr)
        return False


def generate_test_report(test_results):
    """ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
    print("\n=== ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ ===")
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'test_results': test_results,
        'summary': {
            'total_tests': len(test_results),
            'passed_tests': sum(1 for result in test_results.values() if result),
            'failed_tests': sum(1 for result in test_results.values() if not result)
        }
    }
    
    # æˆåŠŸç‡ã®è¨ˆç®—
    if report['summary']['total_tests'] > 0:
        success_rate = (report['summary']['passed_tests'] / report['summary']['total_tests']) * 100
        report['summary']['success_rate'] = f"{success_rate:.1f}%"
    else:
        report['summary']['success_rate'] = "0.0%"
    
    # ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
    report_file = Path('tests/reports/integration_test_report.json')
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«å‡ºåŠ›
    print(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {report['summary']['total_tests']}")
    print(f"æˆåŠŸ: {report['summary']['passed_tests']}")
    print(f"å¤±æ•—: {report['summary']['failed_tests']}")
    print(f"æˆåŠŸç‡: {report['summary']['success_rate']}")
    
    # è©³ç´°çµæœ
    print("\nè©³ç´°çµæœ:")
    for test_name, result in test_results.items():
        status = "âœ“" if result else "âœ—"
        print(f"  {status} {test_name}")
    
    print(f"\nè©³ç´°ãƒ¬ãƒãƒ¼ãƒˆ: {report_file}")
    
    return report


def cleanup_test_environment():
    """ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    print("\nãƒ†ã‚¹ãƒˆç’°å¢ƒã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ä¸­...")
    
    # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
    temp_dir = Path('tests/temp')
    if temp_dir.exists():
        import shutil
        shutil.rmtree(temp_dir, ignore_errors=True)
        temp_dir.mkdir(exist_ok=True)
    
    # ãƒ†ã‚¹ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
    config_file = Path('tests/test_config.json')
    if config_file.exists():
        config_file.unlink()
    
    print("âœ“ ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='Qt-Theme-Studioçµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--workflow', action='store_true', help='å®Œå…¨ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ')
    parser.add_argument('--accessibility', action='store_true', help='ã‚¢ã‚¯ã‚»ã‚·ãƒ“ãƒªãƒ†ã‚£ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ')
    parser.add_argument('--suite', action='store_true', help='çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®ã¿å®Ÿè¡Œ')
    parser.add_argument('--performance', action='store_true', help='ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®ã¿å®Ÿè¡Œ')
    parser.add_argument('--all', action='store_true', help='ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰')
    parser.add_argument('--no-cleanup', action='store_true', help='ãƒ†ã‚¹ãƒˆå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—')
    
    args = parser.parse_args()
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆã‚’å®Ÿè¡Œ
    if not any([args.workflow, args.accessibility, args.suite, args.performance]):
        args.all = True
    
    print("Qt-Theme-Studio çµ±åˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 50)
    
    start_time = time.time()
    
    try:
        # ãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        setup_test_environment()
        
        # ãƒ†ã‚¹ãƒˆçµæœã‚’è¨˜éŒ²
        test_results = {}
        
        # å„ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ
        if args.workflow or args.all:
            test_results['complete_workflow'] = run_complete_workflow_tests()
        
        if args.accessibility or args.all:
            test_results['accessibility_features'] = run_accessibility_tests()
        
        if args.suite or args.all:
            test_results['integration_suite'] = run_integration_suite()
        
        if args.performance or args.all:
            test_results['performance_tests'] = run_performance_tests()
        
        if args.all:
            test_results['all_integration'] = run_all_integration_tests()
        
        # ãƒ†ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ
        report = generate_test_report(test_results)
        
        # å®Ÿè¡Œæ™‚é–“ã®è¡¨ç¤º
        end_time = time.time()
        duration = end_time - start_time
        print(f"\nç·å®Ÿè¡Œæ™‚é–“: {duration:.2f}ç§’")
        
        # çµæœã«åŸºã¥ãçµ‚äº†ã‚³ãƒ¼ãƒ‰
        if all(test_results.values()):
            print("\nğŸ‰ ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
            exit_code = 0
        else:
            print("\nâŒ ä¸€éƒ¨ã®ãƒ†ã‚¹ãƒˆãŒå¤±æ•—ã—ã¾ã—ãŸ")
            exit_code = 1
        
    except KeyboardInterrupt:
        print("\n\nãƒ†ã‚¹ãƒˆå®Ÿè¡ŒãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        exit_code = 130
    except Exception as e:
        print(f"\näºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        exit_code = 1
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if not args.no_cleanup:
            cleanup_test_environment()
    
    sys.exit(exit_code)


if __name__ == '__main__':
    main()